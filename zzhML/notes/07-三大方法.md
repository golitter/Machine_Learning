如何获得测试结果 --》 评估方法

如何评估性能优劣  --》 性能度量

如何判断实质差别  --》 比较检验



# 评估方法

**测试集应该与训练集“互斥”**。

## 常见方法

- 留出法（hold-out）

  拥有的数据集分为**训练集**和**测试集**。

  注意：

  - 保持数据分布一致性（分层采样）
  - 多次重复划分（多次随机划分）
  - 测试集 不能太大、太小（例如：$\frac1 5$ $ \frac 1 3$）

- 交叉验证法（cross validation）

  >  留出法可能导致有些数据一直没有用上。

  k-折交叉验证法：数据集分成k块，用k-1块训练，1块测试，一直循环...直到都测试过（，测试结果取平均值。

  > 若k=m，则得到”留一法“（leave-one-out，LOO）

  

- 自助法（bootstrap）

  基于”自助采样“（bootstrap sampling），也称为”有放回采用“、”可重复采样“。

  约有36.8%的样本不出现。用没有出现的样本进行”包外估计“（out-of-bag estimation）

  - 训练集与原样本集同规模
  - 数据分布有所改变。

# ”调参“与最终模型

算法的参数：一般由人工设定，也称为”超参数“（例如，方程次数）

模型的参数：一般由学习确定（算法确定对应方程次数的模型值）

调参过程相似：先产生若干模型，然后基于某种评估方法进行选择。

验证集：训练集中的一部分，专门用来调参数的部分。

> 算法参数选定后，要用”训练集 + 验证集“ 重新训练最终模型。



# 性能度量

性能度量（performance measure）是衡量模型泛化能力的评价标准，反应了任务需求。使用不同的性能度量往往会导致不同的评判结果。模型好坏，不仅取决于算法和数据，还取决于**任务需求**。

- 回归任务常用均分误差：
  $$
  E(f;D) = \frac 1 m \sum_{i = 1} ^ m(f(x_i) - y_i)^2
  $$
  

分类问题

- 错误率：
  $$
  E(f;D) = \frac 1 m \sum_{i = 1} ^ m I(f(x_i) \ne y_i)
  $$

- 精度：
  $$
  acc(f;D) = 1 - E(f;D)
  $$
  

分类问题可以得到一个”分类结果混淆矩阵“

| 真实情况 | 正例         | 反例         |
| -------- | ------------ | ------------ |
| 正例     | TP（真正例） | FN（假反例） |
| 反例     | FP（假正例） | TN（真反例） |

- 查准率：
  $$
  P = \frac {TP} {TP + FP}
  $$

- 查全率：
  $$
  R = \frac {TP} {TP + FN}
  $$

- F1度量：
  $$
  \frac 1 {F1} = \frac 1 2 \times (\frac 1 P + \frac 1 R)
  $$

# 比较检验

在某种度量下取得评估结果后，不可以直接评判优劣。因为：

- 测试性能不等于泛化性能
- 测试性能随着测试集的变化而变化
- 很多机器学习算法本身有一定的随机性

统计假设检验（hypothesis test）为学习性能比较提供了重要依据。

两学习器比较：

- 交叉验证t检验（基于成对t检验）
  - k折交叉验证；5x2交叉验证
- McNemar检验（基于列联表，卡方检验）

> 2024年11月18日 13点32分 没学明白

