线性模型（linear model）试图学得一个通过属性的线性组合来进行预测的函数。
$$
f(x) = w_1 \times x_1 + w_2 \times x_2 + ... + w_d \times x_d + b
$$

# 线性回归（linear regression）

$$
f(x_i) = w \times x_i + b \\
使得 \ f(x_i) \approx y_i
$$

> 离散属性的处理：
>
> 若有“序”，则连续化；
>
> 否则，转为k维向量。

令均方误差最小化，有
$$
(w*,b*) = arg \ min \sum_{i = 1} ^ {m} (f(x_i) - y_i)^2 \\
arg \ min \sum_{i = 1}^{m} (y_i - w \times x_i - b) ^ 2
$$
对：
$$
E(w,b) = \sum_{i = 1} ^{m} (y_i - w \times x_i - b) ^ 2
$$
进行最小二乘参数估计。

分别对$w$和$b$求偏导：
$$
\frac {\partial E(w,b)} {\partial w} = 2(w \times \sum_{i = 1} ^ {m} x_i^2 \ - \ \sum_{i = 1} ^ {m}(y_i - b)\times x_i) \\
\frac {\partial E(w,b)} {\partial b} = 2 (m \times b \ - \ \sum_{i = 1} ^ {m} (y_i - w \times x_i))
$$
令导数为0，得到闭式解：
$$
w = \frac {\sum_{i=1}^{m}y_i(x_i - \overline{x})} {\sum_{i = 1} ^ {m}x_i^2 - \frac 1 m (\sum_{i=1}^mx_i^2)} \\
b = \frac 1 m \sum_{i=1}^m(y_i - w \times x_i)
$$

# 多元（Multi-variate）线性回归

$$
f(x_i) = {\vec w}^T{\vec x_i} + b \
使得 \ f(\vec {x_i}) \approx y_i \\
\vec {x} = (x_{i1}...x_{id})
$$



将$\vec w$和$b$吸收入向量形式$\hat{\vec w} = ({\vec w}, b)$，数据集表示为：
$$
X = 
\begin{bmatrix}
{x_{11}}&{a_{12}}&{\cdots}&{x_{1d}}\\
{x_{21}}&{a_{22}}&{\cdots}&{x_{2d}}\\
{\vdots}&{\vdots}&{\ddots}&{\vdots}\\
{x_{m1}}&{a_{m2}}&{\cdots}&{x_{md}}\\
\end{bmatrix} = 
\begin{bmatrix}
{\vec x_1}^T & 1 \\
{\vec x_2}^T & 1 \\
{\vdots} & {\vdots} \\
{\vec x_m}^T & 1
\end{bmatrix} \\
{\vec y} = (y_1...y_m)
$$
$$

同样采用最小二乘法求解，有

> @TODO: 公式

![image-20241119105406876](3-1-%E7%BA%BF%E6%80%A7%E5%9B%9E%E5%BD%92.assets/image-20241119105406876.png)

此时需求助于归纳偏好，或引入**正则化**。

# 广义线性模型

**令预测值逼近$y$的衍生物。**

>  ${ln}_{y} = {\vec w}^T {\vec x} + b$，对数线性回归。

一般形式：$y = g^{-1}({\vec w} ^ T {\vec x} + b)$ 

单调可微的**联系函数**（link function）

令$g(.) = ln(.)$则得到 对数线性回归：
$$
{ln}_{y} = {\vec w}^T {\vec x} + b
$$
