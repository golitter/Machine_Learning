信息增益：对可取值数目较多的属性有所偏好。

> 有明显弱点，例如：考虑将“编号”作为一个属性。

增益率（Gain Ratio）：

![image-20241125162459357](40-3-%E5%85%B6%E4%BB%96%E5%B1%9E%E6%80%A7%E5%88%92%E5%88%86%E5%87%86%E5%88%99.assets/image-20241125162459357.png)

启发式：先从候选划分属性中找出信息增益高于平均水平的，再从中选取增益率最高的。

> 规范化，归一化：归一化是规范化的特殊形式。规范化是将不可比的东西可以比较。

基尼指数（Gini Index）：

![image-20241125163205439](40-3-%E5%85%B6%E4%BB%96%E5%B1%9E%E6%80%A7%E5%88%92%E5%88%86%E5%87%86%E5%88%99.assets/image-20241125163205439.png)

### 划分选择 vs. 剪枝

研究表明：划分选择的各种准则虽然对决策树的尺寸有较大影响，但是对泛化性能的影响很有限。例如信息增益与基尼指数产生的结果，仅在约2%的情况下不同。

**剪枝方法和程度**对决策树泛化性能的影响更为显著。在数据带噪时甚至可能将泛化性能提示25%

### 剪枝

为了尽可能正确分类训练样本，有可能造成分支过多 -> 过拟合。**可通过主动去掉一些分支来降低过拟合的风险**

基本策略：

- 预剪枝（pre-pruning）：提取终止某些分支的生长
- 后剪枝（post-pruning）：生成一颗完全树，再“回头”剪枝

